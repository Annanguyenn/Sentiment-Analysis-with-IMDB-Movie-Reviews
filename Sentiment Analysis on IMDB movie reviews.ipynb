{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main steps in the code: \n",
    "1. Use the labeledTrainData.tsv from data folder in a dataframe `train`.\n",
    "2. Build a function to clean the reviews in the input file: review_cleaner(train['review'],lemmatize,stem).\n",
    "3. Build a function to train the sentiment prediction models: train_predict_sentiment(cleaned_reviews, y=train[\"sentiment\"],ngram=1,max_features=1000\n",
    "4. Trained models on unigrams of the reviews without lemmatizing and stemming.\n",
    "5. Trained models on unigrams and bigrams setting of the reviews with lemmatizing and stemming. Then comparing the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# make compatible with Python 2 and Python 3\n",
    "from __future__ import print_function, division, absolute_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data set\n",
    "\n",
    "The labeled training data set consists of 25,000 IMDB movie reviews. There is also an unlabeled test set with 25,000 IMDB movie reviews. The sentiment of the reviews are binary, meaning an IMDB rating < 5 results in a sentiment score of 0, and a rating >=7 have a sentiment score of 1 (no reviews with score 5 or 6 are included in the analysis). No individual movie has more than 30 reviews.\n",
    "\n",
    "## File description\n",
    "\n",
    "* **labeledTrainData** - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review. \n",
    "\n",
    "* **testData** - The unlabeled test set. 25,000 rows containing an id, and text for each review. \n",
    "\n",
    "## Data columns\n",
    "* **id** - Unique ID of each review\n",
    "* **sentiment** - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "* **review** - Text of the review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data set statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "# train.shape should be (25000,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anhnguyen/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "import bs4 as bs\n",
    "import nltk\n",
    "\n",
    "# nltk.download('all')\n",
    "from nltk.tokenize import sent_tokenize  # tokenizes sentences\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "eng_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec3'></div>\n",
    "##  2.Preparing the data set for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function called `review_cleaner` that reads in a review and:\n",
    "\n",
    "- Removes HTML tags (using beautifulsoup)\n",
    "- **Extract emoticons (emotion symbols, aka smileys :D )**\n",
    "- Removes non-letters (using regular expression)\n",
    "- Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n",
    "- Removes all the English stopwords from the list of movie review words\n",
    "- Join the words back into one string seperated by space, append the emoticons to the end\n",
    "\n",
    "(Transform the list of stopwords to a set before removing the stopwords. Use the set to look up stopwords.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def review_cleaner(reviews, lemmatize=True, stem=False):\n",
    "    \"\"\"\n",
    "    Clean and preprocess a review.\n",
    "\n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "    # 1. Remove HTML tags\n",
    "\n",
    "    cleaned_reviews = []\n",
    "    for i, review in enumerate(train[\"review\"]):\n",
    "        # print progress\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(\"Done with %d reviews\" % (i + 1))\n",
    "        review = bs.BeautifulSoup(review).text\n",
    "\n",
    "        # 2. Use regex to find emoticons\n",
    "        emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", review)\n",
    "\n",
    "        # 3. Remove punctuation\n",
    "        review = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "\n",
    "        # 4. Tokenize into words (all lower case)\n",
    "        review = review.lower().split()\n",
    "\n",
    "        # 5. Remove stopwords\n",
    "        eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "        clean_review = []\n",
    "        for word in review:\n",
    "            if word not in eng_stopwords:\n",
    "                if lemmatize is True:\n",
    "                    word = wnl.lemmatize(word)\n",
    "                elif stem is True:\n",
    "                    if word == \"oed\":\n",
    "                        continue\n",
    "                    word = ps.stem(word)\n",
    "                clean_review.append(word)\n",
    "\n",
    "        # 6. Join the review to one sentence\n",
    "\n",
    "        review_processed = \" \".join(clean_review + emoticons)\n",
    "        cleaned_reviews.append(review_processed)\n",
    "\n",
    "    return cleaned_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Function to train and validate a sentiment analysis model using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # CountVectorizer can actucally handle a lot of the preprocessing for us\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics  # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def train_predict_sentiment(\n",
    "    cleaned_reviews, y=train[\"sentiment\"], ngram=1, max_features=1000\n",
    "):\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool, here we show more keywords\n",
    "    vectorizer = CountVectorizer(\n",
    "        ngram_range=(1, ngram),\n",
    "        analyzer=\"word\",\n",
    "        tokenizer=None,\n",
    "        preprocessor=None,\n",
    "        stop_words=None,\n",
    "        max_features=max_features,\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        cleaned_reviews, y, random_state=0, test_size=0.2\n",
    "    )\n",
    "\n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "    #     print('TOP 20 FEATURES ARE: ',(vectorizer.get_feature_names()[:20]))\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 75 trees\n",
    "    forest = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as\n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "\n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\n",
    "        \" The training accuracy is: \",\n",
    "        train_acc,\n",
    "        \"\\n\",\n",
    "        \"The validation accuracy is: \",\n",
    "        valid_acc,\n",
    "    )\n",
    "    print()\n",
    "    print(\"CONFUSION MATRIX:\")\n",
    "    print(\"         Predicted\")\n",
    "    print(\"          neg pos\")\n",
    "    print(\" Actual\")\n",
    "    c = confusion_matrix(y_test, test_predictions)\n",
    "    print(\"     neg  \", c[0])\n",
    "    print(\"     pos  \", c[1])\n",
    "\n",
    "    # Extract feature importnace\n",
    "    print(\"\\nTOP TEN IMPORTANT FEATURES:\")\n",
    "    importances = forest.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_10 = indices[:10]\n",
    "    print([vectorizer.get_feature_names()[ind] for ind in top_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and test  Model on the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.9999 \n",
      " The validation accuracy is:  0.8216\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2102  446]\n",
      "     pos   [ 446 2006]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'great', 'waste', 'awful', 'excellent', 'terrible', 'best', 'boring', 'worse']\n"
     ]
    }
   ],
   "source": [
    "# Here I use the original reviews without lemmatizing and stemming\n",
    "original_clean_reviews = review_cleaner(train[\"review\"], lemmatize=False, stem=False)\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews=original_clean_reviews,\n",
    "    y=train[\"sentiment\"],\n",
    "    ngram=1,\n",
    "    max_features=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 UNIGRAM setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.829\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2138  410]\n",
      "     pos   [ 445 2007]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'great', 'waste', 'awful', 'excellent', 'nothing', 'worse', 'terrible', 'boring']\n"
     ]
    }
   ],
   "source": [
    "# For original reviews with unigram and 1000 max_features:\n",
    "original_clean_reviews = review_cleaner(train[\"review\"], lemmatize=False, stem=False)\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews=original_clean_reviews,\n",
    "    y=train[\"sentiment\"],\n",
    "    ngram=1,\n",
    "    max_features=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.99995 \n",
      " The validation accuracy is:  0.8186\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2095  453]\n",
      "     pos   [ 454 1998]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['worst', 'bad', 'great', 'waste', 'awful', 'excellent', 'boring', 'worse', 'terrible', 'best']\n"
     ]
    }
   ],
   "source": [
    "# For lemmatized reviews with unigram and 1000 max_features:\n",
    "wnl_clean_reviews = review_cleaner(train[\"review\"], lemmatize=True, stem=False)\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews=wnl_clean_reviews, y=train[\"sentiment\"], ngram=1, max_features=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.825\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2109  439]\n",
      "     pos   [ 436 2016]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'wast', 'great', 'aw', 'love', 'excel', 'bore', 'terribl', 'best']\n"
     ]
    }
   ],
   "source": [
    "# For stemmed reviews with unigram and 1000 max_features:\n",
    "ps_clean_reviews = review_cleaner(train[\"review\"], lemmatize=False, stem=True)\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews=ps_clean_reviews, y=train[\"sentiment\"], ngram=1, max_features=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### For original review with unigram and 1000 max_features, I will report:\n",
    "original_clean_reviews=review_cleaner(train['review'],lemmatize=False,stem=False)\n",
    "\n",
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train[\"sentiment\"],ngram=1,max_features=1000)\n",
    "\n",
    "The training accuracy is: 1.0 The validation accuracy is: 0.829\n",
    "\n",
    "### For lemmatized review with unigram and 1000 max_features, I will report:\n",
    "wnl_clean_reviews=review_cleaner(train['review'],lemmatize=True,stem=False)\n",
    "\n",
    "train_predict_sentiment(cleaned_reviews=wnl_clean_reviews, y=train[\"sentiment\"],ngram=1,max_features=1000)\n",
    "\n",
    "The training accuracy is: 0.99995 The validation accuracy is: 0.8186\n",
    "\n",
    "\n",
    "### For stemmed review with unigram and 1000 max_features, I will report:\n",
    "ps_clean_reviews=review_cleaner(train['review'],lemmatize=False,stem=True)\n",
    "\n",
    "train_predict_sentiment(cleaned_reviews=ps_clean_reviews, y=train[\"sentiment\"],ngram=1,max_features=1000)\n",
    "\n",
    "The training accuracy is: 1.0 The validation accuracy is: 0.825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 BIGRAM setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.99995 \n",
      " The validation accuracy is:  0.8182\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2102  446]\n",
      "     pos   [ 463 1989]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['worst', 'bad', 'awful', 'great', 'waste', 'excellent', 'terrible', 'boring', 'worse', 'wonderful']\n"
     ]
    }
   ],
   "source": [
    "# For original reviews with bigram and 1000 max_features:\n",
    "original_clean_reviews = review_cleaner(train[\"review\"], lemmatize=False, stem=False)\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews=original_clean_reviews,\n",
    "    y=train[\"sentiment\"],\n",
    "    ngram=2,\n",
    "    max_features=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.99995 \n",
      " The validation accuracy is:  0.8208\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2124  424]\n",
      "     pos   [ 472 1980]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'great', 'waste', 'awful', 'excellent', 'wonderful', 'boring', 'worse', 'best']\n"
     ]
    }
   ],
   "source": [
    "# For lemmatized reviews with bigram and 1000 max_features:\n",
    "wnl_clean_reviews = review_cleaner(train[\"review\"], lemmatize=True, stem=False)\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews=wnl_clean_reviews, y=train[\"sentiment\"], ngram=2, max_features=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.99995 \n",
      " The validation accuracy is:  0.8236\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2115  433]\n",
      "     pos   [ 449 2003]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'wast', 'great', 'aw', 'excel', 'love', 'bore', 'terribl', 'wast time']\n"
     ]
    }
   ],
   "source": [
    "# For stemmed reviews with bigram and 1000 max_features:\n",
    "ps_clean_reviews = review_cleaner(train[\"review\"], lemmatize=False, stem=True)\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews=ps_clean_reviews, y=train[\"sentiment\"], ngram=2, max_features=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For original review with bigram and 1000 max_features, I will report:\n",
    "original_clean_reviews=review_cleaner(train['review'],lemmatize=False,stem=False)\n",
    "\n",
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train[\"sentiment\"],ngram=2,max_features=1000)\n",
    "\n",
    "The training accuracy is: 0.99995 The validation accuracy is: 0.8182\n",
    "\n",
    "### For lemmatized review with bigram and 1000 max_features, I will report:\n",
    "wnl_clean_reviews=review_cleaner(train['review'],lemmatize=True,stem=False)\n",
    "\n",
    "train_predict_sentiment(cleaned_reviews=wnl_clean_reviews, y=train[\"sentiment\"],ngram=2,max_features=1000)\n",
    "\n",
    "The training accuracy is: 0.99995 The validation accuracy is: 0.8208\n",
    "\n",
    "\n",
    "### For stemmed review with bigram and 1000 max_features, I will report:\n",
    "ps_clean_reviews=review_cleaner(train['review'],lemmatize=False,stem=True)\n",
    "\n",
    "train_predict_sentiment(cleaned_reviews=ps_clean_reviews, y=train[\"sentiment\"],ngram=2,max_features=1000)\n",
    "\n",
    "The training accuracy is: 0.99995 The validation accuracy is: 0.8236"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 UNIGRAM setting for lemmatized reviews with different maximum features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.87155 \n",
      " The validation accuracy is:  0.5588\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [1411 1137]\n",
      "     pos   [1069 1383]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['film', 'movie', 'one', 'good', 'character', 'time', 'like', 'get', 'story', 'even']\n"
     ]
    }
   ],
   "source": [
    "# For lemmatized reviews with unigram, and 10 max_features:\n",
    "wnl_clean_reviews = review_cleaner(train[\"review\"], lemmatize=True, stem=False)\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews=wnl_clean_reviews, y=train[\"sentiment\"], ngram=1, max_features=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.9998 \n",
      " The validation accuracy is:  0.7186\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [1858  690]\n",
      "     pos   [ 717 1735]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'great', 'movie', 'film', 'one', 'best', 'even', 'like', 'love', 'good']\n"
     ]
    }
   ],
   "source": [
    "# For lemmatized reviews with unigram, and 100 max_features\n",
    "wnl_clean_reviews = review_cleaner(train[\"review\"], lemmatize=True, stem=False)\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews=wnl_clean_reviews, y=train[\"sentiment\"], ngram=1, max_features=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8214\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2111  437]\n",
      "     pos   [ 456 1996]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'great', 'awful', 'waste', 'excellent', 'terrible', 'boring', 'wonderful', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "# For lemmatized reviews with unigram, and 1000 max_features\n",
    "wnl_clean_reviews = review_cleaner(train[\"review\"], lemmatize=True, stem=False)\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews=wnl_clean_reviews, y=train[\"sentiment\"], ngram=1, max_features=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8374\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2142  406]\n",
      "     pos   [ 407 2045]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'waste', 'great', 'awful', 'excellent', 'worse', 'best', 'wonderful', 'boring']\n"
     ]
    }
   ],
   "source": [
    "# For lemmatized reviews with unigram, and 5000 max_features\n",
    "wnl_clean_reviews = review_cleaner(train[\"review\"], lemmatize=True, stem=False)\n",
    "\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews=wnl_clean_reviews, y=train[\"sentiment\"], ngram=1, max_features=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For lemmatized reviews with unigram, and 10 max_features, I will report:\n",
    "wnl_clean_reviews=review_cleaner(train['review'],lemmatize=True,stem=False)\n",
    "\n",
    "train_predict_sentiment(cleaned_reviews=wnl_clean_reviews, y=train[\"sentiment\"],ngram=1,max_features=10)\n",
    "\n",
    "The training accuracy is: 0.87155 The validation accuracy is: 0.5588\n",
    "\n",
    "### For lemmatized reviews with unigram, and 100 max_features, I will report:\n",
    "wnl_clean_reviews=review_cleaner(train['review'],lemmatize=True,stem=False)\n",
    "\n",
    "train_predict_sentiment(cleaned_reviews=wnl_clean_reviews, y=train[\"sentiment\"],ngram=1,max_features=100)\n",
    "\n",
    "The training accuracy is: 0.9998 The validation accuracy is: 0.7186\n",
    "\n",
    "### For lemmatized reviews with unigram, and 1000 max_features, I will report:\n",
    "wnl_clean_reviews=review_cleaner(train['review'],lemmatize=True,stem=False)\n",
    "\n",
    "train_predict_sentiment(cleaned_reviews=wnl_clean_reviews, y=train[\"sentiment\"],ngram=1,max_features=1000)\n",
    "\n",
    "The training accuracy is: 1.0 The validation accuracy is: 0.8214\n",
    "\n",
    "### For lemmatized reviews with unigram, and 5000 max_features, I will report:\n",
    "wnl_clean_reviews=review_cleaner(train['review'],lemmatize=True,stem=False)\n",
    "\n",
    "train_predict_sentiment(cleaned_reviews=wnl_clean_reviews, y=train[\"sentiment\"],ngram=1,max_features=5000)\n",
    "\n",
    "The training accuracy is: 1.0 The validation accuracy is: 0.8374"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly observe from the results:\n",
    "- For unigram, while all there reviews give quite similar training accuracy (1.0, 0.99995 and 1.0), the performance in validation accuracy when using the original reviews is the highest compared to those of lemmatized review and stemmed review, which are 0.829, 0.8186 and 0.825 respectively.\n",
    "- For bigram, there is no difference in the training accuracy when using 3 different reviews. However, considering the validation accuracy, lemmatizing and stemming do help improve the performance of the text classification, which is increased from 0.8182 to 0.8208 and 0.8236 respectively.\n",
    "- For unigram with lemmatized reviews, we can noticed that the larger the amount of max features, the better performance in using Random Forest Classifier. Particularly, when changing from 10 to 100 max features, the training accuracy increased 14.7% (from 0.87155 to 0.9998) and the validation accuracy increased significantly, 28.6% (from 0.5588 to 0.7186). And when changing from 100 to 1000 max features, the validation accuracy continued to increase significantly from 0.7186 to 0.8214 (14.3%). \n",
    "\n",
    "Hence, in unigram, the lemmatizing and stemming seem to not really affect the results significantly while in bigram, lemmatizing and stemming do help to improve slightly the classification performance (but not really significant). As for the number of max features, the larger the amount of max features, the better performance of the classification models, especially in the validation accuracy. However, it seems like if we continue increasing the max features beyond 1000, the effect will not be as significant anymore."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
